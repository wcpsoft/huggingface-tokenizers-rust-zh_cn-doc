<p align="center">
    <br>
    <img src="https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png" width="600"/>
    <br>
<p>
<p align="center">
    <img alt="Build" src="https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg">
    <a href="https://github.com/huggingface/tokenizers/blob/main/LICENSE">
        <img alt="GitHub" src="https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue&cachedrop">
    </a>
    <a href="https://pepy.tech/project/tokenizers">
        <img src="https://pepy.tech/badge/tokenizers/week" />
    </a>
</p>

tokenizersæ˜¯å¼€æºçš„åˆ†è¯åº“ï¼Œæä¾›äº†å½“ä»Šæœ€å¸¸ç”¨çš„åˆ†è¯å™¨çš„å®ç°ã€‚

## ä¸»è¦åŠŸèƒ½:

 - ä½¿ç”¨å½“ä»Šæœ€å¸¸ç”¨çš„åˆ†è¯å™¨è®­ç»ƒæ–°çš„è¯æ±‡è¡¨å¹¶è¿›è¡Œåˆ†è¯ã€‚
 - ç”±äºé‡‡ç”¨äº†Rustå®ç°ï¼Œä»¥æå¿«çš„é€Ÿåº¦å®ŒæˆåŒ…æ‹¬è®­ç»ƒå’Œåˆ†è¯ç­‰åŠŸèƒ½ï¼Œæ”¯æŒåœ¨è®¾å¤‡ä¸Šä½¿ç”¨CPUè¿›è¡Œå¤„ç†ï¼Œ1GBæ–‡æœ¬æ‰€éœ€æ—¶é—´ä¸åˆ°20ç§’ã€‚
 - æ˜“äºä½¿ç”¨ï¼ŒåŒæ—¶éå¸¸å¤šåŠŸèƒ½ã€‚
 - ä¸“ä¸ºç ”ç©¶å’Œç”Ÿäº§è®¾è®¡ã€‚
 - å¯ä»¥æ­£åˆ™åŒ–è¿‡ç¨‹ä¸­ä¼šè·Ÿè¸ªå¯¹é½ä¿¡æ¯ã€‚å§‹ç»ˆå¯ä»¥è·å–å¯¹åº”äºç»™å®šä»¤ç‰Œçš„åŸå§‹å¥å­éƒ¨åˆ†ã€‚
 - å¯ä»¥å®Œæˆæ‰€æœ‰é¢„å¤„ç†å·¥ä½œï¼šæˆªæ–­ã€å¡«å……ã€æ·»åŠ æ¨¡å‹æ‰€éœ€çš„ç‰¹æ®Šä»¤ç‰Œç­‰ã€‚

## æ€§èƒ½
æ€§èƒ½å¯èƒ½ä¼šæ ¹æ®ç¡¬ä»¶æœ‰æ‰€ä¸åŒï¼Œä½†åœ¨g6 AWSå®ä¾‹ä¸Šè¿è¡Œ[~/bindings/python/benches/test_tiktoken.py](bindings/python/benches/test_tiktoken.py)åº”è¯¥ä¼šå¾—åˆ°ä»¥ä¸‹ç»“æœï¼š
![image](https://github.com/user-attachments/assets/2b913d4b-e488-4cbc-b542-f90a6c40643d)


## æ”¯æŒçš„è¯­è¨€

æˆ‘ä»¬ä¸ºä»¥ä¸‹è¯­è¨€æä¾›äº†æ”¯æŒï¼ˆè¿˜æœ‰æ›´å¤šè¯­è¨€å³å°†æ”¯æŒï¼‰ï¼š
  - [Rust](https://github.com/huggingface/tokenizers/tree/main/tokenizers)ï¼ˆåŸç”Ÿå®ç°ï¼‰
  - [Python](https://github.com/huggingface/tokenizers/tree/main/bindings/python)
  - [Node.js](https://github.com/huggingface/tokenizers/tree/main/bindings/node)
  - [Ruby](https://github.com/ankane/tokenizers-ruby)ï¼ˆç”±@ankaneè´¡çŒ®ï¼Œå¤–éƒ¨ä»“åº“ï¼‰
 
## ä½¿ç”¨Pythonçš„å¿«é€Ÿå…¥é—¨ï¼š

é€‰æ‹©Byte-Pair Encodingã€WordPieceæˆ–Unigramä»»ä¸€ä¸€ç§æ¨¡å‹å®Œæˆåˆ†è¯å™¨å®ä¾‹åŒ–ï¼š

```python
from tokenizers import Tokenizer
from tokenizers.models import BPE

tokenizer = Tokenizer(BPE())
```

ä½ å¯ä»¥è‡ªå®šä¹‰é¢„åˆ†è¯çš„æ–¹å¼ï¼ˆä¾‹å¦‚ï¼Œæ‹†åˆ†æˆå•è¯ï¼‰ï¼š

```python
from tokenizers.pre_tokenizers import Whitespace

tokenizer.pre_tokenizer = Whitespace()
```

ç„¶ååœ¨ä¸€ç»„æ–‡ä»¶ä¸Šè®­ç»ƒä½ çš„åˆ†è¯å™¨åªéœ€ä¸¤è¡Œä»£ç ï¼š

```python
from tokenizers.trainers import BpeTrainer

trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])
tokenizer.train(files=["wiki.train.raw", "wiki.valid.raw", "wiki.test.raw"], trainer=trainer)
```

ä¸€æ—¦ä½ çš„åˆ†è¯å™¨è®­ç»ƒå®Œæˆï¼Œåªéœ€ä¸€è¡Œä»£ç å³å¯ç¼–ç ä»»ä½•æ–‡æœ¬ï¼š
```python
output = tokenizer.encode("Hello, y'all! How are you ğŸ˜ ?")
print(output.tokens)
# ["Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?"]
```

è¯¦æƒ…æŸ¥çœ‹[documentation](https://huggingface.co/docs/tokenizers/index)æˆ– [quicktour](https://huggingface.co/docs/tokenizers/quicktour)è¿›è¡Œæ›´å¤šå­¦ä¹ 